# Backend Environment Variables
# Copy this file to .env and fill in your values

# Server Configuration
PORT=3001

# ============================================
# Provider Selection
# ============================================
# Choose default inference provider: "local" or "deepgram"
# Can be overridden per-request with "provider" form field
DEFAULT_PROVIDER=local

# ============================================
# Deepgram API Configuration (Optional)
# ============================================
# Sign up at https://deepgram.com to get an API key
# Used for cloud-based transcription with high accuracy and confidence scores
# Leave blank to use only LocalAI

DEEPGRAM_API_KEY=your_deepgram_api_key_here
DEEPGRAM_MODEL=nova-2                    # Options: nova-2, nova-3
DEEPGRAM_LANGUAGE=en                     # ISO 639-1 language code

# ============================================
# LocalAI Configuration (Default)
# ============================================
# Used for local transcription with Whisper and summarization with Llama
# Requires LocalAI running via Docker (see README)

LOCALAI_URL=http://localhost:8080
LOCALAI_WHISPER_MODEL=whisper-1          # Whisper model for transcription
LOCALAI_LLM_MODEL=qwen2.5-7b             # LLM model for summarization
