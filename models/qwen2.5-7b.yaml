backend: llama-cpp
context_size: 4096
mmap: true
name: qwen2.5-7b
parameters:
  model: localai-functioncall-qwen2.5-7b-v0.5-q4_k_m.gguf
stopwords:
  - "<|im_end|>"
template:
  chat_message: |
    <|im_start|>{{ .RoleName }}
    {{ .Content }}<|im_end|>
  chat: |
    {{.Input -}}
    <|im_start|>assistant
