backend: llama-cpp
context_size: 4096
mmap: true
name: llama-3.2-3b-instruct
parameters:
  model: llama-3.2-3b-instruct-q8_0.gguf
stopwords:
  - "<|eot_id|>"
  - "<|end_of_text|>"
template:
  chat_message: |
    <|start_header_id|>{{ .RoleName }}<|end_header_id|>

    {{ .Content }}<|eot_id|>
  chat: |
    {{.Input -}}
    <|start_header_id|>assistant<|end_header_id|>

