# LLM Inference Worker Configuration
# Copy this to .env and fill in your API keys

# Deepgram API (for cloud transcription)
DEEPGRAM_API_KEY=your_deepgram_api_key_here

# Ollama (for local summarization)
OLLAMA_HOST=http://localhost:11434

# Future providers (add as needed)
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=
